{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3ylsa7phjL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04530573-1eaf-4de6-c21e-1f2b1482342b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting miceforest\n",
            "  Downloading miceforest-5.6.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from miceforest) (1.21.6)\n",
            "Collecting lightgbm>=3.3.1\n",
            "  Downloading lightgbm-3.3.4-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from miceforest) (0.3.6)\n",
            "Collecting blosc\n",
            "  Downloading blosc-1.11.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m249.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from lightgbm>=3.3.1->miceforest) (0.38.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.8/dist-packages (from lightgbm>=3.3.1->miceforest) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from lightgbm>=3.3.1->miceforest) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.22.0->lightgbm>=3.3.1->miceforest) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.22.0->lightgbm>=3.3.1->miceforest) (1.2.0)\n",
            "Installing collected packages: blosc, lightgbm, miceforest\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed blosc-1.11.1 lightgbm-3.3.4 miceforest-5.6.3\n"
          ]
        }
      ],
      "source": [
        "# !pip install surprise\n",
        "# !pip install category_encoders\n",
        "# !pip install miceforest --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "dnAFK1gJ00IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.core.indexes.extension import Index\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import plotly.express as px\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from category_encoders.target_encoder import TargetEncoder\n",
        "from copy import deepcopy\n",
        "import joblib\n",
        "\n",
        "import miceforest as mf\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import lightgbm as lgb\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "Pn3-lCHViLoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "XYxnSFiRiL-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ZeAkqzBG1a"
      },
      "source": [
        "# Connecting to colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvigGZdBFn7q",
        "outputId": "656a2b7e-65a4-438d-cdf3-15cf9f2fe000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on CoLab\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "else:\n",
        "  print('Not running on CoLab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sag4XfvuBN3g"
      },
      "source": [
        "## Mounting to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFqtOyUfF0Al",
        "outputId": "bed0dbec-5022-43f1-82d2-b81df943115b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeivP-ibBdcT"
      },
      "source": [
        "# Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqmbfOZgGJl3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/WALMART/CAPSTONE project/Data/online_retail_co_ecommerce.csv')\n",
        "df['number_of_reviews'] = df['number_of_reviews'].str.replace(',','')\n",
        "df['number_of_reviews'] = df['number_of_reviews'].astype('float')\n",
        "df[['number_available_in_stock', 'condition']] = df.number_available_in_stock.str.split(expand = True)\n",
        "df['number_available_in_stock'] = df['number_available_in_stock'].astype('float')\n",
        "df['price'] = df['price'].str.split().str[0]\n",
        "df['price'] = df['price'].str.split(' ',1, expand = True)\n",
        "df['price'] = df['price'].str.replace('£','')\n",
        "df['price'] = df['price'].str.replace(',','')\n",
        "df['price'] = df['price'].astype('float')\n",
        "string_count = df[df['average_review_rating'].astype(str).str.contains('out of 5 stars')]\n",
        "df['average_review_rating'] = df['average_review_rating'].str.split().str[0]\n",
        "df['average_review_rating'] = df['average_review_rating'].astype('float')\n",
        "\n",
        "df['description'] = df['description'].fillna('')\n",
        "df['new_desc'] = df['product_name'] + df['description']\n",
        "df['new_desc'] = df['new_desc'].fillna('')\n",
        "\n",
        "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "tfidf_matrix = tf.fit_transform(df['new_desc'])\n",
        "\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "df = df.reset_index()\n",
        "titles = df[['category_and_sub_category']]\n",
        "indices = pd.Series(df.index, index= df['uniq_id']) \n",
        "\n",
        "def get_recommendations(title):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:15]\n",
        "    product = [i[0] for i in sim_scores]\n",
        "    return titles.iloc[product]\n",
        "\n",
        "for i, row in df.loc[df.category_and_sub_category.isnull()].iterrows():\n",
        "  if (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[0,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[0,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[1,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[1,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[2,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[2,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[3,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[3,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[4,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[4,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[5,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[5,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[6,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[6,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[7,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[7,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[8,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[8,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[9,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[9,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[10,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[10,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[11,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[11,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[12,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[12,0]\n",
        "  elif (pd.isna(get_recommendations(df['uniq_id'][i]).iloc[13,0]) == False):\n",
        "    df['category_and_sub_category'][i] = get_recommendations(df['uniq_id'][i]).iloc[13,0]\n",
        "  else:\n",
        "    df['category_and_sub_category'][i] = None\n",
        "\n",
        "def find_between( s, first, last ):\n",
        "  start = s.index( first ) + len( first )\n",
        "  end = s.index( last, start )\n",
        "  return s[start:end]\n",
        "\n",
        "df['new'] = '0'\n",
        "for i, row in df.loc[df.category_and_sub_category.isnull() & (df.product_information.notnull())].iterrows():\n",
        "  if df['product_information'][i].count('#') != 0:\n",
        "    df['new'][i]= find_between(df['product_information'][i], \"#\" , \">\")\n",
        "\n",
        "df['new'] = df.new.str.split('in',1).str[-1]\n",
        "df['new'] = df['new'].str[1:]\n",
        "\n",
        "for i, row in df.loc[df.category_and_sub_category.isnull() & (df.product_information.notnull())].iterrows():\n",
        "  df['category_and_sub_category'][i] = df['new'][i]\n",
        "\n",
        "df = df.drop(['new'], axis = 1)\n",
        "\n",
        "cols = np.arange(df.category_and_sub_category.str.split(' > ',expand = True).shape[1])\n",
        "cols = cols.tolist()\n",
        "string = 'cat'\n",
        "cols2 = [str(i) + string for i in cols]\n",
        "df[cols2] = df.category_and_sub_category.str.split(' > ',expand = True)\n",
        "\n",
        "df['customers_who_bought_this_item_also_bought'] = df['customers_who_bought_this_item_also_bought'].str.replace(' ','')\n",
        "df['customers_who_bought_this_item_also_bought'] = df['customers_who_bought_this_item_also_bought'].str.replace('http://www.onlineretail.co/','')\n",
        "cols = np.arange(df.customers_who_bought_this_item_also_bought.str.split('|',expand = True).shape[1])\n",
        "cols = cols.tolist()\n",
        "string = 'together'\n",
        "cols4 = [str(i) + string for i in cols]\n",
        "df[cols4] = df.customers_who_bought_this_item_also_bought.str.split('|',expand = True)\n",
        "df[cols4] = df[cols4].replace('(/).*','', regex = True)\n",
        "df[cols4] = df[cols4].replace('-',' ', regex = True)\n",
        "\n",
        "df['items_customers_buy_after_viewing_this_item'] = df['items_customers_buy_after_viewing_this_item'].str.replace(' ','')\n",
        "df['items_customers_buy_after_viewing_this_item'] = df['items_customers_buy_after_viewing_this_item'].str.replace('http://www.onlineretail.co/','')\n",
        "cols2 = np.arange(df.items_customers_buy_after_viewing_this_item.str.split('|',expand = True).shape[1])\n",
        "cols2 = cols2.tolist()\n",
        "string = 'after'\n",
        "cols6 = [str(i) + string for i in cols2]\n",
        "df[cols6] = df.items_customers_buy_after_viewing_this_item.str.split('|',expand = True)\n",
        "df[cols6] = df[cols6].replace('(/).*','', regex = True)\n",
        "df[cols6] = df[cols6].replace('-',' ', regex = True)\n",
        "\n",
        "df['manufacturer'] = df['manufacturer'].replace(np.nan, 'Unknown')\n",
        "\n",
        "df['new'] = 0\n",
        "for i, row in df.loc[df.price.isnull() & (df.sellers.notnull())].iterrows():\n",
        "  df['new'][i]= find_between(df['sellers'][i], \"£\",\"}\")\n",
        "\n",
        "df['new'] = df['new'].str.replace('\"','')\n",
        "df['new'] = df['new'].str.replace(',','')\n",
        "df['new']= df['new'].str.split().str[0]\n",
        "df['new'] = df['new'].astype('float')\n",
        "\n",
        "for i, row in df.loc[df.price.isnull() & (df.sellers.notnull())].iterrows():\n",
        "  df['price'][i] = df['new'][i]\n",
        "\n",
        "df = df.drop(['new'], axis = 1)\n",
        "\n",
        "df['number_of_reviews'] = df['number_of_reviews'].replace(np.nan, 0)\n",
        "\n",
        "df['number_available_in_stock'] = df['number_available_in_stock'].replace(np.nan, 0)\n",
        "\n",
        "df['average_review_rating'] = df['average_review_rating'].replace(np.nan, 0)\n",
        "\n",
        "df['condition'] = df['condition'].replace(np.nan, 'not_avaiable')\n",
        "\n",
        "df['new'] = '0'\n",
        "for i, row in df.loc[df.number_of_answered_questions.isnull() & (df.customer_questions_and_answers.notnull())].iterrows():\n",
        "  df['new'][i] = df['customer_questions_and_answers'][i].count('//')\n",
        "\n",
        "df['new'] = df['new'].astype('float')\n",
        "\n",
        "for i, row in df.loc[df.number_of_answered_questions.isnull() & (df.customer_questions_and_answers.notnull())].iterrows():\n",
        "  df['number_of_answered_questions'][i] = df['new'][i]\n",
        "\n",
        "df = df.drop(['new'], axis = 1)\n",
        "\n",
        "df['number_of_answered_questions'] = df['number_of_answered_questions'].replace(np.nan, 0)\n",
        "\n",
        "df['number_of_sellers'] = '0'\n",
        "for row in df.loc[df.sellers.notnull()].iterrows():\n",
        "  df['number_of_sellers'] = df['sellers'].str.count('Seller_name')\n",
        "\n",
        "df['number_of_sellers'] = df['number_of_sellers'].astype('float')\n",
        "\n",
        "df['number_of_sellers'] = df['number_of_sellers'].replace(np.nan, 0)\n",
        "\n",
        "scaler = minmax_scale\n",
        "df['number_of_reviews'] = df['number_of_reviews'].replace(0,1)\n",
        "df['scaled_reviews_by_category_log'] = df.groupby('0cat').number_of_reviews.transform(lambda x: np.log10(x.astype(float)))\n",
        "df['scaled_reviews_by_category_after_log'] = df.groupby('0cat').scaled_reviews_by_category_log.transform(lambda x: minmax_scale(x.astype(float)))\n",
        "df = df.drop(['scaled_reviews_by_category_log'], axis = 1)\n",
        "df['popularity'] = df['scaled_reviews_by_category_after_log'] * df['average_review_rating']\n",
        "\n",
        "y = df['number_of_reviews']\n",
        "X = df.drop(['number_of_reviews'], axis = 1)\n",
        "\n",
        "df3 = X[['uniq_id', 'manufacturer', 'price', 'number_available_in_stock', 'number_of_answered_questions',\\\n",
        "          'average_review_rating', 'category_and_sub_category', 'condition', 'number_of_sellers']].copy()\n",
        "\n",
        "df3.set_index(['uniq_id'], inplace = True)\n",
        "\n",
        "cols_to_drop = []\n",
        "cat_encoders = {}\n",
        "cat_enc_columns = []\n",
        "\n",
        "for col in df3.columns:\n",
        "    if df3[col].dtype == 'object':\n",
        "        if df3[col].nunique() < 20:\n",
        "            enc = OneHotEncoder( handle_unknown='ignore', sparse=False)\n",
        "            enc.fit(df3[[col]])\n",
        "            joblib.dump(enc, col + '_enc.joblib')\n",
        "            result = enc.transform(df3[[col]])\n",
        "            ohe_columns = [col+\"_\"+str(x) for x in enc.categories_[0]]\n",
        "            cat_enc_columns = cat_enc_columns + ohe_columns\n",
        "            result_train = pd.DataFrame(result, columns=ohe_columns , index=df3.index)\n",
        "            df3 = pd.concat([df3, result_train.reindex(df3.index)], axis=1 , join='inner')\n",
        "\n",
        "        else:\n",
        "            te=TargetEncoder(handle_missing='value',handle_unknown='value', min_samples_leaf=5, smoothing=0.1)\n",
        "            te.fit(df3[col], y.values)\n",
        "            df3[col+'_te']=te.transform(df3[col])\n",
        "            cat_encoders[col] = [deepcopy(te),\"te\"]\n",
        "            cat_enc_columns.append(col+'_te')\n",
        "        cols_to_drop.append(col)\n",
        "\n",
        "df3.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "kernel = mf.ImputationKernel(df3, datasets=1 )\n",
        "\n",
        "pipe = Pipeline([('impute', kernel)])\n",
        "X_t = pipe.fit_transform(df3, y, impute__iterations=3)\n",
        "X_t['uniq_id'] = X_t.index\n",
        "df['price'] = X_t['price'].values\n",
        "df = df.drop(['index'], axis = 1)\n",
        "df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "df = df.drop(['scaled_reviews_by_category_after_log'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv('/content/drive/MyDrive/WALMART/CAPSTONE project/Data/Temp data/df_after_clean_updated.csv')"
      ],
      "metadata": {
        "id": "2kDOD4devqBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2oYlRtHiMFO",
        "outputId": "a077ff5f-1211-43ee-a0d8-5cf0e3a66f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 42 columns):\n",
            " #   Column                                       Non-Null Count  Dtype  \n",
            "---  ------                                       --------------  -----  \n",
            " 0   uniq_id                                      10000 non-null  object \n",
            " 1   product_name                                 10000 non-null  object \n",
            " 2   manufacturer                                 10000 non-null  object \n",
            " 3   price                                        10000 non-null  float64\n",
            " 4   number_available_in_stock                    10000 non-null  float64\n",
            " 5   number_of_reviews                            10000 non-null  float64\n",
            " 6   number_of_answered_questions                 10000 non-null  float64\n",
            " 7   average_review_rating                        10000 non-null  float64\n",
            " 8   category_and_sub_category                    10000 non-null  object \n",
            " 9   customers_who_bought_this_item_also_bought   8938 non-null   object \n",
            " 10  description                                  10000 non-null  object \n",
            " 11  product_information                          9942 non-null   object \n",
            " 12  product_description                          9349 non-null   object \n",
            " 13  items_customers_buy_after_viewing_this_item  6935 non-null   object \n",
            " 14  customer_questions_and_answers               914 non-null    object \n",
            " 15  customer_reviews                             9979 non-null   object \n",
            " 16  sellers                                      6918 non-null   object \n",
            " 17  condition                                    10000 non-null  object \n",
            " 18  new_desc                                     10000 non-null  object \n",
            " 19  0cat                                         10000 non-null  object \n",
            " 20  1cat                                         9902 non-null   object \n",
            " 21  2cat                                         5615 non-null   object \n",
            " 22  3cat                                         1423 non-null   object \n",
            " 23  4cat                                         86 non-null     object \n",
            " 24  0together                                    8938 non-null   object \n",
            " 25  1together                                    8514 non-null   object \n",
            " 26  2together                                    8193 non-null   object \n",
            " 27  3together                                    7958 non-null   object \n",
            " 28  4together                                    7718 non-null   object \n",
            " 29  5together                                    7362 non-null   object \n",
            " 30  6together                                    16 non-null     object \n",
            " 31  7together                                    16 non-null     object \n",
            " 32  8together                                    16 non-null     object \n",
            " 33  9together                                    16 non-null     object \n",
            " 34  10together                                   15 non-null     object \n",
            " 35  11together                                   15 non-null     object \n",
            " 36  0after                                       6935 non-null   object \n",
            " 37  1after                                       6328 non-null   object \n",
            " 38  2after                                       5827 non-null   object \n",
            " 39  3after                                       5420 non-null   object \n",
            " 40  number_of_sellers                            10000 non-null  float64\n",
            " 41  popularity                                   10000 non-null  float64\n",
            "dtypes: float64(7), object(35)\n",
            "memory usage: 3.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation Function"
      ],
      "metadata": {
        "id": "ziBfSAyZxc8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/WALMART/CAPSTONE project/Data/Temp data/df_after_clean.csv')"
      ],
      "metadata": {
        "id": "lx67e-oo5_Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def recommender(unique_id, manufacturer1, manufacturer2):\n",
        "  \n",
        "\n",
        "\n",
        "  \"\"\"Product Name\"\"\"\n",
        "\n",
        "  a = df[df['uniq_id'] == unique_id]['product_name'].values[0]\n",
        "  aa = \"Product's name: \" + a\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"Most frequent item bought after viewing this product\"\"\"\n",
        "\n",
        "  index = np.int((df.loc[df['uniq_id'].isin([unique_id])].index)[0])\n",
        "  if pd.isna(df['0after'].iloc[index]) == False:\n",
        "    b = (df[cols6].iloc[index , :].mode())[0]\n",
        "    bb = \"Most frequent item bought after viewing this product: \" + b\n",
        "  else:\n",
        "    base5 = pd.DataFrame(df.loc[df['uniq_id'].isin([unique_id])])\n",
        "    base55 = df[df['0after'].notnull()]\n",
        "    temp5 = pd.concat([base5, base55])\n",
        "    temp5 = temp5.reset_index(drop = True)\n",
        "    temp5['new_desc'] = temp5['new_desc'].fillna('')\n",
        "    tf5 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "    tfidf_matrix5 = tf5.fit_transform(temp5['new_desc'])\n",
        "    cosine_sim5 = linear_kernel(tfidf_matrix5, tfidf_matrix5)\n",
        "\n",
        "\n",
        "    titles5 = temp5[['uniq_id']]\n",
        "    indices5 = pd.Series(temp5.index, index= temp5['uniq_id'])\n",
        "    def get_recommendations5(title5):\n",
        "      idx5 = indices5[title5]\n",
        "      sim_scores5 = list(enumerate(cosine_sim5[idx5]))\n",
        "      sim_scores5 = sorted(sim_scores5, key=lambda x: x[1], reverse=True)\n",
        "      sim_scores5 = sim_scores5[1:2]\n",
        "      product5 = [i[0] for i in sim_scores5]\n",
        "      return titles5.iloc[product5]\n",
        "    sim_p_id = get_recommendations5(unique_id).iloc[0,0]\n",
        "    index_s = np.int((df.loc[df['uniq_id'].isin([sim_p_id])].index)[0])\n",
        "    b = (df[cols6].iloc[index_s , :].mode())[0]\n",
        "    bb = \"Most frequent item bought after viewing this product(similar): \" + b\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"Most frequent item bought after buying this product\"\"\"\n",
        "\n",
        "  if pd.isna(df['0together'].iloc[index]) == False:\n",
        "    c = (df[cols4].iloc[index , :].mode())[0]\n",
        "    cc = \"Most frequent item bought after buying this product: \" + c\n",
        "  else:\n",
        "    base6 = pd.DataFrame(df.loc[df['uniq_id'].isin([unique_id])])\n",
        "    base66 = df[df['0together'].notnull()]\n",
        "    temp6 = pd.concat([base6, base66])\n",
        "    temp6 = temp6.reset_index(drop = True)\n",
        "    temp6['new_desc'] = temp6['new_desc'].fillna('')\n",
        "    tf6 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "    tfidf_matrix6 = tf6.fit_transform(temp6['new_desc'])\n",
        "    cosine_sim6 = linear_kernel(tfidf_matrix6, tfidf_matrix6)\n",
        "\n",
        "\n",
        "    titles6 = temp6[['uniq_id']]\n",
        "    indices6 = pd.Series(temp6.index, index= temp6['uniq_id'])\n",
        "    def get_recommendations6(title6):\n",
        "      idx6 = indices6[title6]\n",
        "      sim_scores6 = list(enumerate(cosine_sim6[idx6]))\n",
        "      sim_scores6 = sorted(sim_scores6, key=lambda x: x[1], reverse=True)\n",
        "      sim_scores6 = sim_scores6[1:2]\n",
        "      product6 = [i[0] for i in sim_scores6]\n",
        "      return titles6.iloc[product6]\n",
        "    sim_p_id_t = get_recommendations6(unique_id).iloc[0,0]\n",
        "    index_s_t = np.int((df.loc[df['uniq_id'].isin([sim_p_id_t])].index)[0])\n",
        "    c = (df[cols4].iloc[index_s_t , :].mode())[0]\n",
        "    cc = \"Most frequent item bought after buying this product(similar): \" + c \n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"Most popular item in same category and sub category\"\"\"\n",
        "  \n",
        "  category = df['category_and_sub_category'][df.loc[df['uniq_id'].isin([unique_id])].index[0]]\n",
        "  temp = pd.DataFrame(df.loc[df['category_and_sub_category'].isin([category])])\n",
        "  temp = temp[temp['number_available_in_stock'] > 0]\n",
        "\n",
        "  if temp.empty == False:\n",
        "    d = temp['product_name'].loc[temp['popularity'].idxmax()]\n",
        "    dd = \"Most popular product in same category: \" + d\n",
        "  else:\n",
        "    category88 = df['0cat'][df.loc[df['uniq_id'].isin([unique_id])].index[0]]\n",
        "    temp88 = pd.DataFrame(df.loc[df['0cat'].isin([category88])])\n",
        "    temp88 = temp88[temp88['number_available_in_stock'] > 0]\n",
        "    d = temp88['product_name'].loc[temp88['popularity'].idxmax()]\n",
        "    dd = \"Most popular product in Main category: \" + d\n",
        "\n",
        "  \"\"\"Most similar product based on description\"\"\"\n",
        "\n",
        "  base111 = df[df['number_available_in_stock'] > 0] \n",
        "  base222 = df.loc[df['uniq_id'] == unique_id]\n",
        "  temp22 = pd.concat([base111, base222]).drop_duplicates()\n",
        "  temp22 = temp22.reset_index(drop = True)\n",
        "  temp22['new_desc'] = temp22['new_desc'].fillna('')\n",
        "  tf22 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "  tfidf_matrix22 = tf22.fit_transform(temp22['new_desc'])\n",
        "  cosine_sim22 = linear_kernel(tfidf_matrix22, tfidf_matrix22)\n",
        "\n",
        "\n",
        "  titles22 = temp22[['product_name']]\n",
        "  indices22 = pd.Series(temp22.index, index= temp22['uniq_id'])\n",
        "  def get_recommendations22(title22):\n",
        "    idx22 = indices22[title22]\n",
        "    sim_scores22 = list(enumerate(cosine_sim22[idx22]))\n",
        "    sim_scores22 = sorted(sim_scores22, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores22 = sim_scores22[1:3]\n",
        "    product22 = [i[0] for i in sim_scores22]\n",
        "    return titles22.iloc[product22]\n",
        "  \n",
        "  e = get_recommendations22(unique_id).iloc[0,0]\n",
        "  ee = \"Most similar product to this item is: \" + e\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"Similar products from same manufacturer\"\"\"\n",
        "\n",
        "  manufac = df['manufacturer'][df.loc[df['uniq_id'].isin([unique_id])].index[0]]\n",
        "  base112 = pd.DataFrame(df.loc[df['manufacturer'].isin([manufac])])\n",
        "  base113 = base112[base112['number_available_in_stock'] > 0] \n",
        "  temp2 = pd.concat([base113, base112]).drop_duplicates()\n",
        "  temp2 = temp2.reset_index(drop = True)\n",
        "  temp2['new_desc'] = temp2['new_desc'].fillna('')\n",
        "  tf2 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "  tfidf_matrix2 = tf2.fit_transform(temp2['new_desc'])\n",
        "  cosine_sim2 = linear_kernel(tfidf_matrix2, tfidf_matrix2)\n",
        "\n",
        "\n",
        "  titles2 = temp2[['product_name']]\n",
        "  indices2 = pd.Series(temp2.index, index= temp2['uniq_id'])\n",
        "  def get_recommendations2(title2):\n",
        "    idx2 = indices2[title2]\n",
        "    sim_scores2 = list(enumerate(cosine_sim2[idx2]))\n",
        "    sim_scores2 = sorted(sim_scores2, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores2 = sim_scores2[1:6]\n",
        "    product2 = [i[0] for i in sim_scores2]\n",
        "    return titles2.iloc[product2]\n",
        "  mm = []\n",
        "  for i in range(0,len(get_recommendations2(unique_id))):\n",
        "    mm[i] = mm.append(i)\n",
        "    mm[i] = get_recommendations2(unique_id).iloc[i,0]\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"Similar products from promoted manufacturer #1\"\"\"\n",
        "\n",
        "  base1 = pd.DataFrame(df.loc[df['manufacturer'].isin([manufacturer1])])\n",
        "  base1 = base1[base1['number_available_in_stock'] > 0]\n",
        "  base2 = df.loc[df['uniq_id'] == unique_id]\n",
        "  temp3 = pd.concat([base1, base2])\n",
        "\n",
        "  if temp3.empty == False:\n",
        "    temp3 = temp3.reset_index(drop = True)\n",
        "    temp3['new_desc'] = temp3['new_desc'].fillna('')\n",
        "    tf3 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "    tfidf_matrix3 = tf3.fit_transform(temp3['new_desc'])\n",
        "    cosine_sim3 = linear_kernel(tfidf_matrix3, tfidf_matrix3)\n",
        "\n",
        "\n",
        "    titles3 = temp3[['product_name']]\n",
        "    indices3 = pd.Series(temp3.index, index= temp3['uniq_id'])\n",
        "    def get_recommendations3(title3):\n",
        "      idx3 = indices3[title3]\n",
        "      sim_scores3 = list(enumerate(cosine_sim3[idx3]))\n",
        "      sim_scores3 = sorted(sim_scores3, key=lambda x: x[1], reverse=True)\n",
        "      sim_scores3 = sim_scores3[1:3]\n",
        "      product3 = [i[0] for i in sim_scores3]\n",
        "      return titles3.iloc[product3]\n",
        "    mm3 = []\n",
        "    for i in range(0,len(get_recommendations3(unique_id))):\n",
        "      mm3[i] = mm3.append(i)\n",
        "      mm3[i] = get_recommendations3(unique_id).iloc[i,0]\n",
        "  else:\n",
        "    mm3 ='\"{}\" has no available products'.format(manufacturer1)\n",
        "\n",
        "\n",
        "  \n",
        "    \"\"\"Similar products from promoted manufacturer #2\"\"\"\n",
        "\n",
        "  base11 = pd.DataFrame(df.loc[df['manufacturer'].isin([manufacturer2])])\n",
        "  base11 = base11[base11['number_available_in_stock'] > 0]\n",
        "  base22 = df.loc[df['uniq_id'] == unique_id]\n",
        "  temp4 = pd.concat([base11, base22])\n",
        "\n",
        "  if temp4.empty == False:\n",
        "    temp4 = temp4.reset_index(drop = True)\n",
        "    temp4['new_desc'] = temp4['new_desc'].fillna('')\n",
        "    tf4 = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "    tfidf_matrix4 = tf4.fit_transform(temp4['new_desc'])\n",
        "    cosine_sim4 = linear_kernel(tfidf_matrix4, tfidf_matrix4)\n",
        "\n",
        "\n",
        "    titles4 = temp4[['product_name']]\n",
        "    indices4 = pd.Series(temp4.index, index= temp4['uniq_id'])\n",
        "    def get_recommendations4(title4):\n",
        "      idx4 = indices4[title4]\n",
        "      sim_scores4 = list(enumerate(cosine_sim4[idx4]))\n",
        "      sim_scores4 = sorted(sim_scores4, key=lambda x: x[1], reverse=True)\n",
        "      sim_scores4 = sim_scores4[1:3]\n",
        "      product4 = [i[0] for i in sim_scores4]\n",
        "      return titles4.iloc[product4]\n",
        "    mm4 = []\n",
        "    for i in range(0,len(get_recommendations4(unique_id))):\n",
        "      mm4[i] = mm4.append(i)\n",
        "      mm4[i] = get_recommendations4(unique_id).iloc[i,0]\n",
        "\n",
        "  else:\n",
        "    mm4 ='\"{}\" has no available products'.format(manufacturer2)\n",
        "\n",
        "\n",
        "  \n",
        "  return aa, bb, cc, dd, ee,'Other similar products that \"{}\" produces are:'.format(manufac), mm,\\\n",
        "  'Other similar products that \"{}\" produces are:'.format(manufacturer1), mm3,\\\n",
        "  'Other similar products that \"{}\" produces are:'.format(manufacturer2), mm4"
      ],
      "metadata": {
        "id": "9jy1lhceiMNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommender('85e27ae7027190252fd4f5e3fde85eb0', 'Turtles', 'Halo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2qDWVfaiMQB",
        "outputId": "e1950fc1-8128-4d5b-f025-3a1082f0cfe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Product's name: Minecraft Figures (Pack of 3)\",\n",
              " 'Most frequent item bought after viewing this product: Minecraft Core Animal Mob Pack',\n",
              " 'Most frequent item bought after buying this product: John Adams Moshi Monsters Waggle',\n",
              " 'Most popular product in same category: Playmobil 5494 Christmas Advent Calendar Santas Workshop',\n",
              " 'Most similar product to this item is: Minecraft Cow/Steve with Pickaxe and Spider Figures (Pack of 3)',\n",
              " 'Other similar products that \"Minecraft\" produces are:',\n",
              " ['Minecraft Cow/Steve with Pickaxe and Spider Figures (Pack of 3)',\n",
              "  'Minecraft Core Animal Mob (Pack of 6)',\n",
              "  'Minecraft 3-inch Alex Figure with Accessory',\n",
              "  'Minecraft Steve ~6\" Vinyl Figure'],\n",
              " 'Other similar products that \"Turtles\" produces are:',\n",
              " ['Turtles Raph with Mini Cycle Half-Shell Heroes (Pack of 2)',\n",
              "  'Turtles Shredder and Foot Soldier Half-Shell Heroes (Pack of 2)'],\n",
              " 'Other similar products that \"Halo\" produces are:',\n",
              " ['HALO 5 Guardians Series 1 Spartan Variant Action Figure',\n",
              "  'Halo Reach Warthog Accessory Gauss Cannon Rocket Launcher with Spartan JFO Custom Operator Figure'])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ln_SvXDGaSSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4w33vRf6aSN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PKriiFeJaR_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# from here is just to test"
      ],
      "metadata": {
        "id": "eaboWzaHyRXo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsd9HclAYkjn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
